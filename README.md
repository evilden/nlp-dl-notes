# nlp-dl-notes
A collection of useful notes for DL in NLP
## Word Vector Representations

* [Word Vector Representations (word2vec)](https://youtu.be/8rXD5-xhemo)
* [Word Vectors and Word Senses](https://youtu.be/kEMJRjEdNzM) (0:00-38:40, 58:00-1:20:00)
* Additional materials:
  * [CS224n: Natural Language Processing with Deep Learning](https://youtu.be/OQQ-W_63UgQ)
  * [CS224n: Word Vector Representations, word2vec](https://youtu.be/ERibwqs9p38)
  * [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model)
  * [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)
  * [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)
 
### Word2Vec
   - https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf
   - https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes02-wordvecs2.pdf
   - http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/
   - https://arxiv.org/pdf/1301.3781.pdf (original word2vec paper)
   - https://towardsdatascience.com/word2vec-from-scratch-with-numpy-8786ddd49e72 (w2v from scratch)
   - Negative sampling:
    - http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf (negative sampling paper)
    - http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ 
   
## Neural Networks. Backpropagation
 
 * [CS224n: Word Vectors and Word Senses](https://youtu.be/kEMJRjEdNzM) (38:40-58:00)
 * [Enriching Word Vectors with Subword Information](https://www.mitpressjournals.org/doi/pdfplus/10.1162/tacl_a_00051)
 * [CS231n: Backpropagation, Neural Networks 1](https://youtu.be/i94OvYb6noo)
 * Additional materials:
    * [CS224n: GloVe: Global Vectors for Word Representation](https://youtu.be/ASn7ExxLZws)
    * [fastText](https://youtu.be/CHcExDsDeHU)
    
## Neural Networks. Initialization and Normalization
 * [CS231n: Neural Networks](https://www.youtube.com/watch?v=gYpoJMlgyXA)
 * [CS231n: Lecture Notes](http://cs231n.github.io/neural-networks-1)
 * [CS231n: Lecture Notes](http://cs231n.github.io/neural-networks-2)
  
## Neural Networks. Optimization

 * [CS231n: Neural Networks](https://www.youtube.com/watch?v=hd_KFJ5ktUc)
 * [CS231n: Lecture Notes](http://cs231n.github.io/neural-networks-3)

## Recurrent Neural Networks and Language Models

 * [CS224n: Language Models and RNNs](https://youtu.be/iWea12EAu6U)
 * [CS224n: Language Models and RNNs: Notes](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf)
 
## Vanishing Gradients, Fancy RNNs
* [CS224n: Vanishing Gradients, Fancy RNNs](https://www.youtube.com/watch?v=QEw0qEa0E50&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=7)
* [Paper: On the difficulty of training recurrent neural networks](http://proceedings.mlr.press/v28/pascanu13.pdf)
* [Article: Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

## Convolutional Networks for NLP
* [CS224n: Convolutional Networks for NLP](https://youtu.be/EAJoRA0KX7I)
* [Paper: Natural Language Processing (Almost) from Scratch](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf)
* [Paper: Comparative Study of CNN and RNN for Natural Language Processing](https://arxiv.org/abs/1702.01923)
* [Paper: Convolutional Neural Networks for Sentence Classification](https://www.aclweb.org/anthology/D14-1181.pdf)
   
## Translation, Seq2Seq, Attention
* [CS224n: Translation, Seq2Seq, Attention](https://www.youtube.com/watch?v=XXtpJxZBa2c)
* [Lecture Notes](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes06-NMT_seq2seq_attention.pdf)
* [Paper: Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)
* [Paper: Effective approaches to attention-based neural machine translation](https://arxiv.org/pdf/1508.04025)
* [Article: Attention? Attention! by Lilian Weng](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)
